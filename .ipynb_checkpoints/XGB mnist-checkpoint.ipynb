{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from funcs.utils import *\n",
    "from funcs.explainNumpy import *\n",
    "import shap\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from funcs.DIMV import DIMVImputation\n",
    "from funcs.miss_forest import mf\n",
    "\n",
    "missing_rate = 0.2\n",
    "nruns = 1\n",
    "ouput_name = 'XGBRegressor_mnist_rate02'  \n",
    "\n",
    "chosen_model = xgboost.XGBRegressor(n_estimators=100, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((8,8)),  # Resizing to 8x8 to match input size of 64 (8x8)\n",
    "                                transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='../../data', train=True,  transform=transform, download=False)\n",
    "test_dataset = datasets.MNIST(root='../../data', train=False, transform=transform,  download=False)\n",
    "\n",
    "X_train = np.array([train_dataset[i][0].numpy() for i in range(len(train_dataset))])\n",
    "y_train = np.array([train_dataset[i][1] for i in range(len(train_dataset))])\n",
    "\n",
    "X_test = np.array([test_dataset[i][0].numpy() for i in range(len(test_dataset))])\n",
    "y_test = np.array([test_dataset[i][1] for i in range(len(test_dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((-1,64))\n",
    "X_test = X_test.reshape((-1,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    # X_train = pd.DataFrame(scaler.transform(X_train), columns = X_train.columns)\n",
    "    # X_test = pd.DataFrame(scaler.transform(X_test), columns = X_train.columns)\n",
    "\n",
    "    y_train, y_test = y_train.reshape((-1,1)), y_test.reshape((-1,1))\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_y.fit(y_train)\n",
    "    # y_train = pd.DataFrame(scaler_y.transform(y_train))\n",
    "    # y_test = pd.DataFrame(scaler_y.transform(y_test))    \n",
    "    X_train_star = generate_missing_data(X_train, rate=missing_rate)\n",
    "    X_test_star = generate_missing_data(X_test, rate=missing_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate missing data, impute, and use SHAP to explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thung\\.conda\\envs\\mtime\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:825: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Cross Validation with alphas = [0.0, 0.01, 0.1, 1.0, 10.0, 100.0] and 100.0 % of training set\n",
      "Running Cross Validation, alpha=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████████████████████████████████████▎   | 61/64 [16:54:31<48:48, 976.09s/it]"
     ]
    }
   ],
   "source": [
    "results = one_run(X_train, X_train_star, y_train, X_test, X_test_star, y_test, chosen_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the first layer of results is the result of each run. Then, for the sub-level:\n",
    "- level 0: shape_values_ori, shap_values_xm...\n",
    "- level 1:  other_measures = [mse_imputation_all, mse_shap_all, mse_ypred_all, cor_ypred_all, spearman_ypred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_shap_vals(results, j, nruns):\n",
    "    # get the average shap values from all runs for each imputation method or the original \n",
    "    current = results[0][0][j]\n",
    "    for i in range(1, nruns):\n",
    "        current.values += results[i][0][j].values\n",
    "        current.base_values += results[i][0][j].base_values\n",
    "        current.data += results[i][0][j].data  \n",
    "    current.values = current.values/nruns\n",
    "    current.base_values = current.base_values/nruns\n",
    "    current.data = current.data/nruns\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_ori = get_average_shap_vals(results, j = 0, nruns= nruns)\n",
    "shap_values_xm = get_average_shap_vals(results, j = 1, nruns= nruns)\n",
    "shap_values_mi = get_average_shap_vals(results, j = 2, nruns = nruns)\n",
    "shap_values_mice = get_average_shap_vals(results, j = 3, nruns= nruns)\n",
    "shap_values_dimv = get_average_shap_vals(results, j = 4, nruns= nruns)\n",
    "shap_values_mf = get_average_shap_vals(results, j = 5, nruns= nruns)\n",
    "shap_values_soft = get_average_shap_vals(results, j = 6, nruns= nruns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranking_res = shap_ranking_table(X_train, shap_values_ori,shap_values_xm, shap_values_mi, shap_values_mice,\n",
    "                   shap_values_dimv, shap_values_mf, shap_values_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    j = 0\n",
    "    mse_now = results[0][1][j]\n",
    "    for i in range(1, nruns):\n",
    "        mse_now += results[i][1][j]\n",
    "    mse_imputation_all = mse_now/nruns  \n",
    "    print(\"the MSE between the imputed X_test of mean imputation, MICE, DIMV, MissForest, SOFT-IMPUTE and the original X_test:\")\n",
    "    print(mse_imputation_all.round(3))\n",
    "\n",
    "    j = 1\n",
    "    mse_now = results[0][1][j]\n",
    "    for i in range(1, nruns):\n",
    "        mse_now += results[i][1][j]    \n",
    "    mse_shap_all = mse_now/nruns \n",
    "    print(\"the MSE between the Shapley values of mean imputation, MICE, DIMV, MissForest, SOFT-IMPUTE and the original:\")\n",
    "    print(mse_shap_all.round(3))\n",
    "\n",
    "    j = 2\n",
    "    mse_ypred_now = results[0][1][j]\n",
    "    for i in range(1, nruns):\n",
    "        mse_ypred_now += results[i][1][j]    \n",
    "    mse_ypred_all = mse_ypred_now/nruns \n",
    "    print(\"the MSE between y predicted on test set of mean imputation, MICE, DIMV, MissForest, SOFT-IMPUTE and the original:\")\n",
    "    print(mse_ypred_all.round(3))\n",
    "\n",
    "    j = 3\n",
    "    mse_ypred_now = results[0][1][j]\n",
    "    for i in range(1, nruns):\n",
    "        mse_ypred_now += results[i][1][j]    \n",
    "    spearman_ypred = mse_ypred_now/nruns \n",
    "    print(\"the ranking correlation for spearman rank correlation between the predicted y on test set of original data and y predicted on imputed data\")\n",
    "    print(spearman_ypred.round(3))\n",
    "\n",
    "    j = 4\n",
    "    mse_ypred_now = results[0][1][j]\n",
    "    for i in range(1, nruns):\n",
    "        mse_ypred_now += results[i][1][j]    \n",
    "    spearman_ypred = mse_ypred_now/nruns \n",
    "    print(\"the ranking correlation for spearman rank correlation between imputed features\")\n",
    "    print(spearman_ypred.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfplot = pd.DataFrame({'MSE':np.hstack((0,mse_imputation_all)), 'Spearman statistic': spearman_ypred[:,0]}, index = ['Xgb on missing data','Mean Imputation','MICE','DIMV','missForest','SOFT-IMPUTE'])\n",
    "dfplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('imputation MSE versus Spearman statistic')\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(dfplot['MSE'], dfplot['Spearman statistic'], color='skyblue')\n",
    "for i, row in dfplot.iterrows():\n",
    "    if row.name == 'MICE':\n",
    "        plt.text(row['MSE'],row['Spearman statistic'], row.name, ha='left', va='bottom')\n",
    "    else:\n",
    "        plt.text(row['MSE'],row['Spearman statistic'], row.name, ha='right', va='bottom') \n",
    "plt.ylabel('Spearman statistic')\n",
    "plt.xlabel('MSE')\n",
    "plt.xlim(-.35, 0.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)  # Adding grid for better visualization\n",
    "plt.savefig('results/'+ ouput_name+'imputation_mse_spearman'+'.png',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfplot = pd.DataFrame({'MSE on y test':mse_ypred_all, 'MSE Shap': mse_shap_all}, index = dfplot.index)\n",
    "dfplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('imputation MSE versus Shapley MSE')\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(dfplot['MSE on y test'],dfplot['MSE Shap'], color='skyblue')\n",
    "for i, row in dfplot.iterrows():\n",
    "    plt.text(row['MSE on y test'],row['MSE Shap'], row.name, ha='center', va='bottom')\n",
    "plt.ylabel('MSE Shap')\n",
    "plt.xlabel('MSE on y test')\n",
    "plt.xlim(0.08, 0.15)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)  # Adding grid for better visualization\n",
    "plt.savefig('results/'+ ouput_name+'imputation_mse_vs_shap_mse'+'.png',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfplot = pd.DataFrame({'MSE':np.hstack((0,mse_imputation_all)), 'MSE Shap': mse_shap_all}, index = dfplot.index)\n",
    "dfplot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
